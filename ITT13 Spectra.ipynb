{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "attached-colors",
   "metadata": {},
   "source": [
    "# ITT 13 Spectra - Machine Learning for Natural Language to SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-tribune",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extensive-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "import shlex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-california",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "finnish-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(text):\n",
    "    \"\"\" Splits a text string into basic tokens \"\"\"\n",
    "    if isinstance(text, list):\n",
    "        # If input is a list of strings then apply recursively to each element in list\n",
    "        return [split(t) for t in text]\n",
    "    try:\n",
    "        return shlex.split(text, posix = False)\n",
    "    except:\n",
    "        return ''.join(text)\n",
    "\n",
    "def stemmer(stemmer, y_tokens):\n",
    "    \"\"\" Wraps a stemming function to not split quotes \"\"\"\n",
    "    def _stem(tokens):\n",
    "        ret = []\n",
    "        for token in tokens:\n",
    "            if '\"' in token or \"'\" in token or token in y_tokens:\n",
    "                ret.append(token)\n",
    "            else:\n",
    "                ret.append(stemmer(token))\n",
    "        return split(' '.join(ret))\n",
    "    return _stem\n",
    "\n",
    "def preprocess(data, stem = False):\n",
    "    \"\"\" Returns a list of 2D numpy arrays each corresponding to a text input \"\"\"\n",
    "    unique = set()\n",
    "    for d in data:\n",
    "        unique |= set(stem(d)) if stem else set(d)\n",
    "    encode    = {k : v for v, k in enumerate(unique)}\n",
    "    processed = []\n",
    "    for d in data:\n",
    "        observation = []\n",
    "        tokens      = stem(d) if stem else d\n",
    "        for token in tokens:\n",
    "            vector = np.zeros(len(encode) + 1)\n",
    "            vector[encode[token]] = 1\n",
    "            observation.append(vector)\n",
    "        processed.append(np.array(observation))\n",
    "    return processed, np.array(list(unique) + ['']) # Blank to denote empty string for x or end of query for y\n",
    "\n",
    "def tidy(X, limit):\n",
    "    \"\"\" To make all data points be the same length by padding it (not neccessary but easier computations) \"\"\"\n",
    "    ret = np.zeros((len(X), limit, X[0].shape[1]))\n",
    "    for i, x in enumerate(X):\n",
    "        for r, c in zip(*np.where(x == 1)):\n",
    "            ret[i, r, c] = 1\n",
    "        for j in range(len(x), limit):\n",
    "            ret[i, j, -1] = 1\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-denver",
   "metadata": {},
   "source": [
    "## Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "small-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_others.json') as js:\n",
    "    data = json.load(js)\n",
    "    \n",
    "# Which data base to train on\n",
    "# {'academic', 'geo', 'imdb', 'restaurants', 'scholar', 'yelp'}\n",
    "# [181, 564, 109, 125, 569, 111] (number of data points in each db_id)\n",
    "db_id = 'geo'\n",
    "\n",
    "x_q   = [] # question\n",
    "y_sql = [] # sql\n",
    "\n",
    "for d in data:\n",
    "    if d['db_id'] == db_id:\n",
    "        x_q.append(split(d['question']))\n",
    "        y_sql.append(split(d['query']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-underwear",
   "metadata": {},
   "source": [
    "## Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hollywood-saturday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'the', 'biggest', 'city', 'in', 'wyoming']\n",
      "['what', 'is', 'the', 'biggest', 'city', 'in', 'wyom', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "Y, y_tokens = preprocess(y_sql)\n",
    "\n",
    "stem        = stemmer(nltk.PorterStemmer().stem, y_tokens)\n",
    "\n",
    "X, x_tokens = preprocess(x_q, stem)\n",
    "\n",
    "max_len = max(map(len, X))\n",
    "limit   = (max_len // 10 + 1) * 10 # Pad X such that each observation is max_len rounded up to the nearest 10\n",
    "X       = tidy(X, limit)\n",
    "\n",
    "# Check first data point if it has been tokenised and padded appropriately\n",
    "print(x_q[0])\n",
    "print(x_tokens[X[0].argmax(axis = 1)].tolist()) # padded version with empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-consent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-taxation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
